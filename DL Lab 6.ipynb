{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentationsNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.4.1 requires fsspec, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from albumentations) (1.10.1)\n",
      "Collecting scikit-image>=0.21.0 (from albumentations)\n",
      "  Using cached scikit_image-0.21.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Collecting pydantic>=2.7.0 (from albumentations)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting albucore==0.0.17 (from albumentations)\n",
      "  Downloading albucore-0.0.17-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting eval-type-backport (from albumentations)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from albumentations) (4.11.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.7.0->albumentations)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.7.0->albumentations)\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.9.0 (from albumentations)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached PyWavelets-1.4.1-cp38-cp38-win_amd64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading albumentations-1.4.18-py3-none-any.whl (224 kB)\n",
      "Downloading albucore-0.0.17-py3-none-any.whl (10 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.9 MB 3.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.3/14.9 MB 2.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.4/14.9 MB 3.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.4/14.9 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.9 MB 4.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.8/14.9 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.9 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.4/14.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.4/14.9 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.5/14.9 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/39.4 MB 4.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 2.1/39.4 MB 5.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.1/39.4 MB 5.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.9/39.4 MB 4.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.5/39.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.8/39.4 MB 5.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/39.4 MB 5.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 8.9/39.4 MB 5.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.7/39.4 MB 5.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.3/39.4 MB 5.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 12.6/39.4 MB 5.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 17.3/39.4 MB 5.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 18.6/39.4 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/39.4 MB 5.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 23.6/39.4 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 25.2/39.4 MB 5.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.2/39.4 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.5/39.4 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.1/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 31.5/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.0/39.4 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.6/39.4 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.6/39.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.9/39.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.0/39.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/39.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.1 MB/s eta 0:00:00\n",
      "Using cached scikit_image-0.21.0-cp38-cp38-win_amd64.whl (22.7 MB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached PyWavelets-1.4.1-cp38-cp38-win_amd64.whl (4.2 MB)\n",
      "Using cached tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "Installing collected packages: typing-extensions, numpy, lazy_loader, eval-type-backport, tifffile, PyWavelets, pydantic-core, opencv-python-headless, imageio, annotated-types, scikit-image, pydantic, albucore, albumentations\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed PyWavelets-1.4.1 albucore-0.0.17 albumentations-1.4.18 annotated-types-0.7.0 eval-type-backport-0.2.2 imageio-2.35.1 lazy_loader-0.4 numpy-1.24.4 opencv-python-headless-4.11.0.86 pydantic-2.10.6 pydantic-core-2.27.2 scikit-image-0.21.0 tifffile-2023.7.10 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs453tkgdf3h",
    "outputId": "f4e53f6f-2c33-413e-ec98-c7fc4825ccce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\anaconda3\\envs\\con_env_2\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from albumentations import HorizontalFlip, RandomBrightnessContrast, Resize, Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svh-kWQJk5Z7",
    "outputId": "2328ed8b-b5b9-41f4-f8eb-07914807048f"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'archive.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m extract_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcityscapes\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Folder where files will be extracted\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Open the ZIP file and extract its contents\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m      9\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(extract_to)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\con_env_2\\lib\\zipfile.py:1253\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'archive.zip'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_path = \"archive.zip\"\n",
    "extract_to = \"cityscapes\"  # Folder where files will be extracted\n",
    "\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5HChtDsKjDF8"
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_DIR = r\"C:\\Users\\abdul\\Downloads\\Deep Learning\\Datasets\\Cityscapes\"\n",
    "\n",
    "train_images = os.path.join(DATASET_DIR, r\"C:\\Users\\abdul\\Downloads\\Deep Learning\\Datasets\\Cityscapes\\train\\img\")\n",
    "train_labels = os.path.join(DATASET_DIR, r\"C:\\Users\\abdul\\Downloads\\Deep Learning\\Datasets\\Cityscapes\\train\\label\")\n",
    "\n",
    "val_images = os.path.join(DATASET_DIR, r\"C:\\Users\\abdul\\Downloads\\Deep Learning\\Datasets\\Cityscapes\\val\\img\")\n",
    "val_labels = os.path.join(DATASET_DIR, r\"C:\\Users\\abdul\\Downloads\\Deep Learning\\Datasets\\Cityscapes\\val\\label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jkiwlk1DnN4V"
   },
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, lbl_dir, augmentations=None):\n",
    "\n",
    "        # Path to the folder containing images & masks\n",
    "        self.img_dir = img_dir\n",
    "        self.lbl_dir = lbl_dir\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        # Store the sorted list of filenames for images and masks to ensure they are correctly paired\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.labels = sorted(os.listdir(lbl_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Returns the number of images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx]) # Get image path\n",
    "        lbl_path = os.path.join(self.lbl_dir, self.labels[idx]) # Get label path\n",
    "        img = cv.imread(img_path) # Read the image\n",
    "        lbl = cv.imread(lbl_path, cv.IMREAD_GRAYSCALE) # Read the label in grayscale\n",
    "\n",
    "        # If augmentations are provided\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=img, mask=lbl)\n",
    "            img, lbl = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # divide by 255 to normalize them into the range [0, 1]\n",
    "        img = img.float()/255.0\n",
    "\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V2zjEUHOjG5j"
   },
   "outputs": [],
   "source": [
    "# preprocessing and augmentation\n",
    "def get_augmentations(train=True):\n",
    "    if train:\n",
    "        return Compose([\n",
    "            Resize(96, 256),  # Resize images to (256, 512)\n",
    "            HorizontalFlip(p=0.5), # Flip images horizontally with 50% probability\n",
    "            RandomBrightnessContrast(p=0.2), # Randomly change brightness/contrast with 20% probability\n",
    "            ToTensorV2() # Convert image/mask to PyTorch tensors\n",
    "        ])\n",
    "    else: # No data augmentation for validation (Only preprocessing)\n",
    "        return Compose([\n",
    "            Resize(96, 256),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8eQRQzD9m-L3"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_augmentations = get_augmentations(train=True)\n",
    "val_augmentations = get_augmentations(train=False)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_labels, augmentations=train_augmentations)\n",
    "val_dataset = SegmentationDataset(val_images, val_labels, augmentations=val_augmentations)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l0TSk9NtmYdJ"
   },
   "outputs": [],
   "source": [
    "# Define the Fully Convolutional Network (FCN)\n",
    "# VGG16 as a feature extractor (encoder) & custom decoder to reconstructs the segmentation\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        backbone = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # remove fully connected layers & keep only convolutional layers\n",
    "        self.encoder = backbone.features\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "          # nn.ConvTranspose2d(Input channels, Output channels, kernel_size, stride, padding, output_padding)\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # reduce the number of channels to match the number of segmentation classes\n",
    "            nn.Conv2d(64, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    # x = input image [batch_size, channels, height, width]\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Extracts features using VGG16 (Encoder)\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Passing features through the Decoder\n",
    "        segmentation_map = self.decoder(features)\n",
    "        segmentation_map = nn.functional.interpolate(segmentation_map, size=(96, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpviHHKkoAQp",
    "outputId": "e4f8dbe8-700c-449b-ae68-a7d763dd5527"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\abdul/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [01:43<00:00, 5.34MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FCN(num_classes=19)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95SteuPBoXN5",
    "outputId": "51899447-00dc-4617-af74-e2d8fe830e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8yTFCPreoZZm"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IBMVKMQobKI",
    "outputId": "057261a7-2e13-4d5e-f757-274e596c69ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0974\n",
      "Epoch [2/10], Loss: 0.0949\n",
      "Epoch [3/10], Loss: 0.0939\n",
      "Epoch [4/10], Loss: 0.0913\n",
      "Epoch [5/10], Loss: 0.0906\n",
      "Epoch [6/10], Loss: 0.0882\n",
      "Epoch [7/10], Loss: 0.0866\n",
      "Epoch [8/10], Loss: 0.0854\n",
      "Epoch [9/10], Loss: 0.0849\n",
      "Epoch [10/10], Loss: 0.0851\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "num_classes = 19\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "\n",
    "        # Ensurs the pixel values in masks are in the range [0, num_classes - 1].\n",
    "        masks = torch.clamp(masks, 0, num_classes-1)\n",
    "        images, labels = images.float().to(device), masks.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbNrNjzTod0v",
    "outputId": "5cfa5800-4770-4b78-ca4e-682168a242fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'semantic_segmentation_model.pt')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU_ZZoi1xV1Z",
    "outputId": "2cdf7a53-6fef-4f8f-c23f-be65d340941f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 97.86%\n"
     ]
    }
   ],
   "source": [
    "# Validation loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            masks = torch.clamp(masks, 0, num_classes-1)\n",
    "            images, masks = images.float().to(device), masks.long().to(device)\n",
    "\n",
    "            outputs = model(images)  # Model prediction (logits)\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Get class with highest probability\n",
    "\n",
    "            correct_pixels += (predictions == masks).sum().item()\n",
    "            total_pixels += masks.numel()  # Total number of pixels\n",
    "\n",
    "accuracy = correct_pixels / total_pixels * 100  # Convert to percentage\n",
    "print(f\"Final Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
